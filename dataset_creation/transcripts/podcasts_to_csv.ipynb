{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39fc508",
   "metadata": {},
   "source": [
    "# Method of gathering podcasts:\n",
    "1) Compile list of top podcast channels in Apple store (there were 24)\n",
    "2) Generate 10 random numbers between 1-24, select those channels (repeats ok)\n",
    "3) For each of those 10, select a random podcast from their most recent 10 episodes (as of 4/4/24).\n",
    "\n",
    "Some challenges: not all the originally selected podcasts had transcripts or sometimes the transcripts were not formatted in an ideal way.\n",
    "\n",
    "I'm considering pulling additional podcasts from lemonada due to its ideal formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5d6d0",
   "metadata": {},
   "source": [
    "# Podcast episodes that are converted to .csv so far\n",
    "\n",
    "* MisSpelling: <a href = \"https://omny.fm/shows/misspelling/miss-spelling-part-1\" > Part 1 </a>\n",
    "* Stuff You Should Know: <a href = \"https://omny.fm/shows/stuff-you-should-know-1/short-stuff-mariko-aoki-phenomenon\" > Short Stuff: Mariko Aoki Phenomenon </a>\n",
    "* 20/20: <a href = \"https://www.happyscribe.com/public/20-20/highway-hunter\" > Highway Hunter </a>\n",
    "* The Sarah Silverman Podcast: <a href = \"https://lemonadamedia.com/podcast/flaps-canters-kid-actors/\" > Flaps, Canterâ€™s, Kid Actors </a>\n",
    "* Last Day <a href = \"https://lemonadamedia.com/podcast/christine-in-our-crisis-we-have-opportunity/\" > Christine: In Our Crisis, We Have Opportunity </a>\n",
    "* The Deep Dive with Jessica St. Clair and... <a href = \"https://lemonadamedia.com/podcast/cat-scratch-fever/\" > Cat Scratch Fever </a>\n",
    "* Add to Cart with Kulap Vilaysack &... <a href = \"https://lemonadamedia.com/podcast/the-wax-man-with-lamar-woods/\" > The Wax Man (with Lamar Woods) </a>\n",
    "\n",
    "**Need to spot check above transcripts for reliability**\n",
    "* MissSpelling is pretty bad, but could potentially be fixed by merging neighboring strings together.\n",
    "\n",
    "## Podcasts whose format would require RegEx \n",
    "(as opposed to relying on line breaks)\n",
    "* ZOE:<a href=\"https://zoe.com/learn/podcast-longevity-according-to-blue-zones#transcript\"> 9 longevity practices: Secrets from the blue zones</a>\n",
    "* Serial: <a href = \"https://www.nytimes.com/interactive/2024/podcasts/serial-season-four-guantanamo.html\" > Poor Baby Raul </a>\n",
    "* Serial: <a href = \"https://serialpodcast.org/season-three/1/transcript\" > A Bar Fight Walks into the Justice Center </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d02c34",
   "metadata": {},
   "source": [
    "# Steps for importing chunked transcripts\n",
    "1) Copy/paste transcripts into transcripts.py (formatted as strings)\n",
    "2) Manually determine the line_freq and line_hit by counting lines\n",
    "3) Run raw_transcript_to_csv on each transcript with the corresponding line_freq and line_hit values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c49e0",
   "metadata": {},
   "source": [
    "# Convert English transcripts to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b37a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce5d8",
   "metadata": {},
   "source": [
    "## Original function, which does not account for max_char_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a875951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_transcript_to_csv(transcript, transcript_name, line_freq, line_hit):\n",
    "    \"\"\"\n",
    "    required packages: csv and os\n",
    "    transcript: transcript file\n",
    "    transcript_name: desired name for the string in the .csv\n",
    "    line_freq: the number of lines in a cycle (i.e. \"name/time, new line, quote, name/time...\" has line_freq = 3)\n",
    "    line_hit: the index in the cycle with the desired line of text (*first line has index 0)\n",
    "    \"\"\"\n",
    "    spoken_parts = []\n",
    "    lines = transcript.strip().split('\\n')\n",
    "\n",
    "# for transcripts from lemonada, the format of lines follows the pattern: \"name time\", \"\", \"spoken_part\", \"\", \"name time\"...\n",
    "# so it looks like quotes will always be in the same position of each cycle of lines\n",
    "# I'll use index%%line_freq == line_hit to extract the correct line for the spoken_part\n",
    "\n",
    "    for index in range(len(lines)):\n",
    "        if index%line_freq == line_hit:\n",
    "            spoken_parts.append(lines[index].strip())\n",
    "\n",
    "# Save spoken parts into a CSV file\n",
    "    csv_file = os.path.join(\"English Transcript CSVs\", transcript_name + \"_spoken_parts.csv\")\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        for spoken_part in spoken_parts:\n",
    "            writer.writerow([spoken_part])\n",
    "    \n",
    "    message = (\"Spoken parts have been saved to:\", csv_file)\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff1dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def eng_transcript_to_csv_alt(transcript, transcript_name, line_freq, line_hit):\n",
    "    \"\"\"\n",
    "    transcript: transcript file\n",
    "    transcript_name: desired name for the string in the .csv\n",
    "    line_freq: the number of lines in a cycle (i.e. \"name/time, new line, quote, name/time...\" has line_freq = 3)\n",
    "    line_hit: the index in the cycle with the desired line of text (*first line has index 0)\n",
    "    \"\"\"\n",
    "    # Initialize list to store starting indices\n",
    "    starting_indices = []\n",
    "    \n",
    "    spoken_parts = []\n",
    "    lines = transcript.strip().split('\\n')\n",
    "\n",
    "    # Extract spoken parts based on line frequency and line hit\n",
    "    for index in range(len(lines)):\n",
    "        if index % line_freq == line_hit:\n",
    "            spoken_parts.append(lines[index].strip())\n",
    "            starting_indices.append(index)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(spoken_parts, columns=['Spoken_Part'])\n",
    "    \n",
    "    # Save DataFrame to CSV file\n",
    "    csv_file = os.path.join(\"English Transcript CSVs\", transcript_name + \"_spoken_parts.csv\")\n",
    "    df.to_csv(csv_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    message = (\"Spoken parts have been saved to:\", csv_file, starting_indices)\n",
    "\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ff463",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_transcript_to_csv(Flaps, Eng_podcasts[podcast][0], Eng_podcasts[podcast][1], Eng_podcasts[podcast][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a05333",
   "metadata": {},
   "source": [
    "Transcripts were saved as a **.py** file in the same directory.  They are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25883220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transcripts_english import Flaps, Christine, Mariko, Highway, Cat, Wax, Alyssa, Carrie, Unfit, Awkward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbaa79",
   "metadata": {},
   "source": [
    "Set up a dictionary to loop over with the transcript to csv function.  The dictionary values are formatted as *[\"name\", line_freq, line_hit]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ce81dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Eng_podcasts = {Flaps:[\"Flaps\",6,2], Christine:[\"Christine\",6,2], Mariko:[\"Mariko\",5,2],\n",
    "            Highway:[\"Highway\",3,0], Cat:[\"Cat\",6,2], Wax:[\"Wax\",6,2], Alyssa:[\"Alyssa\",6,2], Carrie:[\"Carrie\",4,2],\n",
    "               Unfit:[\"Unfit\",4,2], Awkward:[\"Awkward\",4,2]}\n",
    "\n",
    "for podcast in Eng_podcasts:\n",
    "    eng_transcript_to_csv(podcast, Eng_podcasts[podcast][0], Eng_podcasts[podcast][1], Eng_podcasts[podcast][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed3f53",
   "metadata": {},
   "source": [
    "# Convert Spanish Transcripts to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237a34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa88b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotify_transcript_to_csv(transcript, transcript_name):\n",
    "    \"\"\"\n",
    "    required packages: csv, re, and os\n",
    "    transcript: transcript file\n",
    "    transcript_name: desired name for the string in the .csv\n",
    "    \"\"\"\n",
    "    # for transcripts from spotify, the speakers swap on timestamps with inconsistent # of lines in between\n",
    "\n",
    "    timestamp_pattern = r'\\b\\d{1,2}:\\d{2}\\b'\n",
    "\n",
    "    lines = re.split(timestamp_pattern, transcript)\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    lines = [' '.join(line.split('\\n')).strip() for line in lines]\n",
    "\n",
    "# Save spoken parts into a CSV file\n",
    "    csv_file = os.path.join(\"Spanish Transcript CSVs\", transcript_name + \"_spoken_parts.csv\")\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        for line in lines:\n",
    "            writer.writerow([line])\n",
    "    \n",
    "    message = (\"Spoken parts have been saved to:\", csv_file)\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c88ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transcripts_spanish import Barberias, Techos, Amponas, Damitas, Subio, Daniel, Ovnis, Fobias, Farid, Triangulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52d7564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp_podcasts = {Barberias:\"Barberias\", Techos:\"Techos\", Amponas:\"Amponas\", Damitas:\"Damitas\", Subio:\"Subio\", Daniel:\"Daniel\", Ovnis:\"Ovnis\", Fobias:\"Fobias\", Farid:\"Farid\", Triangulo:\"Triangulo\"}\n",
    "\n",
    "for podcast in Sp_podcasts:\n",
    "    spotify_transcript_to_csv(podcast, Sp_podcasts[podcast])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
